
# Subword Regularization with consistency objective

```
USER_DIR=<home>/AbuGlyphPerb/fairseq/
DATABIN=<path to your biniarized data directory>
RAWTRAIN=<path to untokenized data of source language>
SPM=<path to a pretrained sentencepiece model>
MODEL=<path to save the model>
WORKER=16

src=<source language>
trg=<target language>

fairseq-train \
  $DATABIN \
  --user-dir $USER_DIR \
  --task augmented-translation \
  --unprocessed-data $RAWTRAIN \
  --save-dir $MODEL \
  --source-lang $src --target-lang $trg \
  --src-tokenizer-type sentencepiece --src-tokenizer-path $SPM \
  --sentencepiece-alpha 0.2 --sentencepiece-nbest -1 \
  --src-tokenizer-num-workers $WORKER --fp16 \
  --criterion label_smoothed_cross_entropy_js \
  --js-alpha 0.6 --js-warmup 2000 --js-temp 1 --js-type mvsr \
  --arch transformer --share-all-embeddings \
  --encoder-layers 5 --decoder-layers 5 \
  --encoder-embed-dim 512 --decoder-embed-dim 512 \
  --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \
  --encoder-attention-heads 2 --decoder-attention-heads 2 \
  --encoder-normalize-before --decoder-normalize-before \
  --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \
  --weight-decay 0.0001 \
  --label-smoothing 0.2 \
  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \
  --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \
  --lr 1e-3 --stop-min-lr 1e-9 \
  --max-tokens 4000 \
  --max-epoch 1000 --no-epoch-checkpoints \
  --eval-bleu \
  --eval-bleu-args '{"beam": 5, "lenpen": 1.2}' \
  --eval-bleu-detok moses \
  --eval-bleu-remove-bpe=sentencepiece \
  --eval-bleu-print-samples \
  --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \
  --ddp-backend=legacy_ddp 
```

# IGE/DGE + SR with consistency objective

```
USER_DIR=<home>/AbuGlyphPerb/fairseq/
DATABIN=<path to your biniarized data directory>
RAWTRAIN=<path to untokenized data of source language>
SPM=<path to a pretrained sentencepiece model>
NOISES=<path to a model generated by ../perturbation/get_glyph_similarity.py, e.g., ige_similarity.npz>
MODEL=<path to save the model>
WORKER=16

src=<source language>
trg=<target language>

fairseq-train \
  $DATABIN \
  --user-dir $USER_DIR \
  --task augmented-translation \
  --unprocessed-data $RAWTRAIN \
  --save-dir $MODEL \
  --source-lang $src --target-lang $trg \
  --src-tokenizer-type similarity_aware_ns \
  --src-tokenizer-noises-path $NOISES \
  --src-tokenizer-path $SPM \
  --noise-prob 1.0 --src-tokenizer-noises-min-score 1.0 \
  --sentencepiece-alpha 0.2 --sentencepiece-nbest -1 \
  --src-tokenizer-num-workers 8 --fp16 \
  --criterion label_smoothed_cross_entropy_js \
  --js-alpha 0.6 --js-warmup 2000 --js-temp 1 --js-type mvsr \
  --arch transformer --share-all-embeddings \
  --encoder-layers 5 --decoder-layers 5 \
  --encoder-embed-dim 512 --decoder-embed-dim 512 \
  --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \
  --encoder-attention-heads 2 --decoder-attention-heads 2 \
  --encoder-normalize-before --decoder-normalize-before \
  --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \
  --weight-decay 0.0001 \
  --label-smoothing 0.2 \
  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0 \
  --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \
  --lr 1e-3 --stop-min-lr 1e-9 \
  --max-tokens 4000 \
  --max-epoch 1000 --no-epoch-checkpoints \
  --eval-bleu \
  --eval-bleu-args '{"beam": 5, "lenpen": 1.2}' \
  --eval-bleu-detok moses \
  --eval-bleu-remove-bpe=sentencepiece \
  --eval-bleu-print-samples \
  --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \
  --ddp-backend=legacy_ddp 
```

# Notes

There could be out-of-memory error. Run `ulimit -n 2048` before `fairseq-train` would solve the problem.